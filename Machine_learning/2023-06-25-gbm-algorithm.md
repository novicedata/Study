---
excerpt: "Gradient Boostingì˜ ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•´ ì•Œì•„ë³´ì."

categories:
  - Machine learning

tags:
  - [Boost]

toc: true
toc_sticky: true

breadcrumb: true

date: 2023-06-26
last_modified_at: 2023-06-26

title: "[Machine learning, ë¨¸ì‹ ëŸ¬ë‹] GBM(Gradient Boosting Model) ì•Œê³ ë¦¬ì¦˜ ë¶„ì„"
---
<br>

ì €ë²ˆ í¬ìŠ¤íŒ…ì— ì´ì–´ **`GBM`**ì˜ ì¢€ë” ìì„¸í•œ ë‚´ìš©ì„ ì„¤ëª…í•˜ë ¤ê³  í•œë‹¤.

Gradient Boostingì˜ ê¸°ë³¸ motivationê³¼ ì•Œê³ ë¦¬ì¦˜ì˜ ìœ ë„ë²•? ë“±ì„ ê³µë¶€í•´ì„œ ì •ë¦¬!

<br>

# â“ ì™œ Gradient ì¸ê°€?

---

![image](https://github.com/novicedata/scrap-comment/assets/88019539/eecfd086-ced7-4830-925c-3e02750b1dad)

ìœ„ì™€ ê°™ì€ ì ë“¤ê³¼ ì„ í˜•ì‹ ê·¸ë¦¬ê³  ì”ì°¨ê°€ ìˆë‹¤ê³  í•©ì‹œë‹¤. GBMì˜ ê¸°ë³¸ì ì¸ motivationì€ ì”ì°¨ë¥¼ yë¡œ ë‘˜ ìˆ˜ ìˆë‹¤ë©´..? ì…ë‹ˆë‹¤.

**ì¦‰ $y-f_1(x)$ì¸ residualì„ ë‘ ë²ˆì§¸ ëª¨ë¸ì¸ $f_2(x)$ë¡œ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ë©´?**

ì´ëŠ” ê³§ **$y-f_1(x) = f_2(x)$**ê°€ ê°€ëŠ¥í•˜ë‹¤ë©´?ì´ ë˜ê² ê³  ë˜í•œ yì— ëŒ€í•´ì„œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. **$y=f_1(x)+f_2(x)$**

ëª¨ë¸ì„ í•™ìŠµí•  ë•Œ ë§ˆë‹¤ ë§ì¶”ì§€ ëª»í•œ ì”ì°¨ ë¶€ë¶„ì„ yê°’ìœ¼ë¡œ ë‘”ë‹¤!

ê·¸ë˜ì„œ ì´ê²Œ ì™œ gradientì™€ ê´€ê³„ê°€ ìˆëŠ”ê°€? ì œì¼ ê¸°ë³¸ì ì¸ **loss function(ì†ì‹¤í•¨ìˆ˜)ì¸ ìµœì†ŒììŠ¹ë²•(Ordinary least square)ë¡œ ìƒê°**í•´ë´…ì‹œë‹¤. í•´ë‹¹ ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê°’ì„ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤.
$$
\min L = \frac12 \sum_{i=1}^n(y_i-f(x_i))^2
$$

ì´ë¥¼ $f(x_i)$ì— ëŒ€í•´ ë¯¸ë¶„í•˜ë©´ $\frac{\partial L}{\partial f(x_i)} = f(x_i) - y_i$ ì´ë˜ê³  ì–‘ ë³€ì— ìŒìˆ˜ë¥¼ ì·¨í•œë‹¤ë©´..?
$$
y_i - f(x_i) = -\frac{\partial L}{\partial f(x_i)}
$$
ì–´ë””ì„œ ë§ì´ ë³¸ ì‹ì´ì£ ? $y_i-f(x_i)$ ì¦‰ ì”ì°¨ì— ëŒ€í•œ ì‹ì´ ë©ë‹ˆë‹¤..

> **`ì”ì°¨`**ë§Œí¼ í•™ìŠµí•˜ë¼ëŠ” ê²ƒ = **`Gradientì˜ ë°˜ëŒ€ ë°©í–¥`**ìœ¼ë¡œ í•™ìŠµí•˜ë¼ì™€ ê°™ì€ ë§ì´ ë©ë‹ˆë‹¤.

<br>

# âš™ï¸ ì•Œê³ ë¦¬ì¦˜ ì„¸ë¶€ ì„¤ëª…

---

## ì•Œê³ ë¦¬ì¦˜

í•´ë‹¹ ì•Œê³ ë¦¬ì¦˜ì˜ ì „ë°˜ì ì¸ ì´í•´ë¥¼ ìœ„í•´ ì „ ê¸€ì„ ì½ê³ ì™€ì£¼ì‹œë©´ ê°ì‚¬í•˜ê² ìŠµë‹ˆë‹¤!

[[Machine learning] GBM(Greg Ridgeway. 2020) ë¦¬ë·° - ë°ì´í„°ë¥¼ íŒŒê³ íŒŒëŠ” ì‚¬ëŒ â›ï¸](https://novicedata.github.io/machine%20learning/gbm-greg/)

**ğŸ’» ì•Œê³ ë¦¬ì¦˜ ì„¤ëª….**

> **$\hat{f}(x) to be constant$ : $\hat{f}(x) = \arg\min_p\sum_{i=1}^N\Psi(y_i,\rho).$**
> 
> **`ë§¨ ì²˜ìŒ. ì‹¤ì œê°’ê³¼ì˜ ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™” í•˜ëŠ” ìƒìˆ˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.`**
> 
> ***for* $t$ in $1,...,T$**
> 
> > ***Step 1.*** ìŒì˜ ê·¸ë ˆë””ì–¸íŠ¸(ì”ì°¨)ë¥¼ ê³„ì‚°
> > 
> > **`í•´ë‹¹ ì‹ì€ ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì†ì‹¤í•¨ìˆ˜ë¥¼ ë¯¸ë¶„í•œ ê°’ì— ìŒìˆ˜ë¥¼ ì·¨í•œ ê°’ìœ¼ë¡œ -(gradient)ì´ì residual(ì”ì°¨)ì…ë‹ˆë‹¤.`**
> > $$ z_i = -\frac{\partial}{\partial f(x_i)}\Psi(y_i,f(x_i))$$
> > 
> > ***Step 2.*** residualì„ targetìœ¼ë¡œ ì¦‰, $z_i$ë¥¼ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ëª¨ë¸ $g(x)$ë¥¼ ì í•©
> > 
> > **`ì˜ˆì¸¡í•˜ëŠ”ë° ì‚¬ìš©ë˜ëŠ” ì…ë ¥ê°’ xë¡œë¶€í„° Step.1ì˜ ìŒì˜ ê·¸ë ˆë””ì–¸íŠ¸ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.`**
> > 
> > ***Step 3.*** ê²½ì‚¬ í•˜ê°•ë²•ì˜ ë‹¨ê³„ í¬ê¸° ì„ íƒ :
> > 
> > $\gamma = \arg \min_{\gamma} \sum^N_{i=1} \Psi(y_i,\hat{f}(x_i) + \gamma g(x_i))$
> > 
> > **`ìŒì˜ ê·¸ë ˆë””ì–¸íŠ¸ ì¦‰, ìŒì˜ ê¸°ìš¸ê¸°ê°’ìœ¼ë¡œ ì–´ëŠ í¬ê¸°ë§Œí¼ ì§„í–‰í•  ê²ƒì¸ì§€ ìœ„ ì‹ì˜ ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ìƒìˆ˜ ê°’ì„ ì°¾ìŠµë‹ˆë‹¤.`**
> > 
> > ***Stpe 4.*** $f(x)$ ì—…ë°ì´íŠ¸
> > 
> > $\hat{f}(x) \leftarrow \hat{f}(x) + \gamma g(x)$
> > 
> > **`ë‹¤ìŒ ëª¨ë¸ì„ ìœ„í•´ ì—…ë°ì´íŠ¸ í•©ë‹ˆë‹¤.`**

ì „ ê¸€ê³¼ ë‹¬ë¼ì§„ ì ì€ $\rho$ë¥¼ $\gamma$ë¡œ ë°”ê¾¼ ê²ƒ ë¿. ë‹¬ë¼ì§„ ì ì€ ì—†ìŠµë‹ˆë‹¤.(ë³´ê¸° í¸í•˜ê¸° ìœ„í•´ ë³€ê²½í•œ ê²ƒ ë¿ì…ë‹ˆë‹¤!)

## 1ï¸âƒ£ ì•Œê³ ë¦¬ì¦˜ 1ë²ˆ. $\hat{f}(x) = \arg\min_{\rho} \sum^n_{i=1} \Psi(y_i,\rho)$

ì´ˆê¸° ëª¨í˜•ì€ ìƒìˆ˜ë¡œ ì„¤ì •í•˜ì—¬ ìœ„ì™€ ê°™ì€ ì‹ìœ¼ë¡œ $\rho$ë¥¼ ì°¾ìŠµë‹ˆë‹¤. $F_0(x)(=f_0(x))$ë¥¼ ì°¾ëŠ” ê²ƒ.

ìš°ì„  ë³´ê¸° í¸í•˜ë„ë¡ $\Psi$ë¥¼ $L$ë¡œ í‘œí˜„í•˜ê² ìŠµë‹ˆë‹¤. í‘œí˜„ë§Œ ë‹¤ë¥¼ ë¿ ê°™ì€ ì†ì‹¤í•¨ìˆ˜(loss function)ì´ë¼ëŠ” ëœ»ì…ë‹ˆë‹¤.

**ê·¸ë¼ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ì•Œê³ ë¦¬ì¦˜ì´ í•´ê²°í•˜ê³ ì í•˜ëŠ” ë¬¸ì œë¥¼ ì¼ë°˜í™”** í•´ë³´ë©´ outputì¸ $y$ì™€ $p$ì°¨ì› input vector $x$ì˜ ì†ì‹¤ í•¨ìˆ˜ $L$ì— ëŒ€í•˜ì—¬ ê¸°ëŒ€ ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™” í•˜ëŠ” í•¨ìˆ˜ $\hat{F} \in H$ë¥¼ ì°¾ê³  ì‹¶ì€ ê²ƒì…ë‹ˆë‹¤.
ì—¬ê¸°ì„œ $H = \{F|F : \mathbb{R}^p \rightarrow \mathbb{R}\}$ ì¦‰. $H$ëŠ” $p$ì°¨ì› ë²¡í„°ë¥¼ ìƒìˆ˜ë¡œ ë°”ê¾¸ëŠ” ê¸°ëŒ€ í•¨ìˆ˜ $\hat{F}$ì˜ ì§‘í•©ì…ë‹ˆë‹¤.

$$
Eq (1). : \hat{F}=\arg\min_{F \in H} E_{x.y}[L(y,F(x))]
$$

ë¬¸ì œëŠ” $H$ê°€ ë„ˆë¬´ í¬ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ë˜ì„œ $F$ë¥¼ ì°¾ëŠ” ê²ƒì´ ì–´ë µì£ . ë•Œë¬¸ì— **`ì œì•½`**ì„ ì¤ë‹ˆë‹¤.

$$
H^* = \{F|F : \mathbb{R}^p \rightarrow \mathbb{R}, F = const. + \gamma_1f_1 + ... + \gamma_Mf_M, f_j \in H, \gamma_j \in R, j=1,...,M\}
$$

ì—¬ê¸°ê°€ ì¤‘ìš”í•œë°, **ìœ„ ì‹ì€ ì¦‰ ì¼ë°˜ í•¨ìˆ˜ê°€ ì•„ë‹Œ ìƒìˆ˜(const)ì™€ ì—¬ëŸ¬ ìœ í•œí•œ í•¨ìˆ˜ì˜ ì„ í˜• ê²°í•©ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” êµ¬ì¡°ë¡œ í‘œí˜„**í•˜ê² ë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.

ì ì•Œì•„ë³¸ $H^*$ì„ ì ìš©í•˜ì—¬ `Eq(1)`ì„ í‘œí˜„í•´ë³´ë©´

$\hat{F} = \arg \min_{F \in H^*}\frac1n \sum_{i=1}^n[L(y,F(x))]$ì´ ë©ë‹ˆë‹¤.

**ì´ë ‡ê²Œ ìœ ë„ëœ $\hat{F}$ëŠ” ì•„ë˜ì™€ ê°™ì€ ìˆœì„œë¡œ ì°¾ê²Œ ë©ë‹ˆë‹¤.**

> $F_0 = \arg \min_{\rho} \sum_{i=1}^n L(y_i, \rho)$
> 
> $F_m = F_{m-1} + \arg \min_{f_m \in H} \sum_{i=1}^n L(y_i, F_{m-1}(x_i)+f_m(x_i)) ... \ Eq(2)$

`ì—¬ê¸°ì„œ ìš°ë¦¬ëŠ” ìµœì†Œí™”í•˜ëŠ” ê°’ì„ ì°¾ëŠ” ê²ƒì´ê¸° ë–„ë¬¸ì— 1/nì€ ë²„ë¦½ë‹ˆë‹¤.`

ìœ„ ì™€ê°™ì€ ì‹ì„ 1ë¶€í„° ëŒ€ì…í•´ë³¸ë‹¤ë©´?

$F_1 = F_0 + \arg \min_{f_1 \in H}\sum_{i=1}^nL(y_i,F_0(x_i) + f_1(x_i))$

$F_2 = F_1 + \arg \min_{f_2 \in H}\sum_{i=1}^nL(y_i,F_1(x_i) + f_2(x_i)) = F_0 + f_1 + \arg \min_{f_2 \in H}\sum_{i=1}^nL(y_i,F_0+ f_1(x_i) + f_2(x_i)) $ê²°êµ­ ì´ë¥¼ ë°˜ë³µí•˜ë©´?

$$
F_M = F_0(const.) + f_1 + f_2+...+f_m
$$

ì´ ë©ë‹ˆë‹¤. ìš°ë¦¬ê°€ ì œì•½ì„ ë‘ì—ˆë˜ ì‹ê³¼ ê°™ì€ ì‹ì´ ë˜ëŠ” ê²ƒì´ì£ ?

> ë˜í•œ, ìœ„ `Eq(2)`ëŠ” **ì•Œê³ ë¦¬ì¦˜ 1ë²ˆì˜ ì‹ê³¼ ê°™ìŠµë‹ˆë‹¤.**

<br>

## 2ï¸âƒ£ ì•Œê³ ë¦¬ì¦˜ 2ë²ˆ. ì”ì°¨ë¥¼ êµ¬í•œë‹¤. $z_i=-\frac{\partial}{\partial f(x_i)}\Psi(y_i,f(x_i))$

ê·¸ë¼ë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ…ì—ì„œëŠ” **`Eq(2)`**ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë° ê²½ì‚¬í•˜ê°•ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.

ì´ ë§ì€.. **$\arg \min_{f_m \in H} \sum_{i=1}^n L(y_i, F_{m-1}(x_i)+f_m(x_i)) = -\gamma\sum_{i=1}^n \nabla L(y_i, F_{m-1}(x_i)+f_m(x_i))$**ë¡œ ê·¼ì‚¬í•œë‹¤ëŠ” ëœ»ì…ë‹ˆë‹¤.

ê·¸ëŸ¼ `Eq(2)`ëŠ” ì´ì™€ ê°™ì•„ ì§‘ë‹ˆë‹¤. 
$$
Eq (2*): F_m = F_{m-1} -\gamma\sum_{i=1}^n \nabla L(y_i, F_{m-1}(x_i)+f_m(x_i))
$$

**`ì—¬ê¸°ì„œ r(ê°ë§ˆ)ëŠ” ê²½ì‚¬í•˜ê°•ë²•ì—ì„œ ì“°ì´ëŠ” stepwise size. ë‹¨ê³„ í¬ê¸°ì…ë‹ˆë‹¤. ë’¤ì—ì„œ ë‚˜ì˜¤ëŠ” í•™ìŠµë¥ ê³¼ëŠ” ë‹¬ë¼ìš”!`**

ë§¨ ì²˜ìŒ ì„¤ëª…í•œ ê²ƒê³¼ ê°™ì´ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ë¯¸ë¶„ìœ¼ë¡œ í‘œí˜„í•˜ë©´ $Eq(3) :\nabla L(y_i, F_{m-1}(x_i)+f_m(x_i)) = \frac{\partial L(y_i,F_{m-1}(x_i)+f_m(x_i))}{\partial f_m(x_i)}$ì™€ ê°™ì´ í‘œí˜„ë˜ëŠ”ë°, ì´ì‹ì„ ì§ì ‘ ê³„ì‚°í•˜ê¸°ëŠ” ì–´ë ¤ì›Œì„œ **1ì°¨ í…Œì¼ëŸ¬ ê·¼ì‚¬ë¥¼ í†µí•´ í‘œí˜„**í•©ë‹ˆë‹¤.
$$
\nabla L(y_i, F_{m-1}(x_i)+f_m(x_i)) \approx L(y_i,F_{m-1}(x_i)) +f_m(x_i)[\frac{\partial L(y_i,F(x_i))}{\partial F(x_i)}]
$$

í›„ ì´ë¥¼ $f_m$ì— ëŒ€í•˜ì—¬ ë¯¸ë¶„í•´ì£¼ë©´? ì•ì„œ ì‚´í´ë³´ì•˜ë˜ `Eq(3)`ì„ ë‹¤ìŒê³¼ ê°™ì´ ê·¼ì‚¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$
\frac{\partial L(y_i,F_{m-1}(x_i)+f_m(x_i))}{\partial f_m(x_i)} \approx [\frac{\partial L(y_i,F(x_i))}{\partial F(x_i)}]
$$

ì´ë¥¼ `Eq(2*)`ì— ëŒ€ì…í•œë‹¤ë©´

> $F_m = F_{m-1} -\gamma \sum_{i=1}^n[\frac{\partial L(y_i,F(x_i))}{\partial F(x_i)}]$

ê·¸ë¦¬ê³  ì—¬ê¸°ì„œ $\gamma$ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ìµœì í™” í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

> $\gamma_m = \arg \min_{\gamma>0}\sum_{i=1}^n L(y_i, F_{m-1}-\gamma \sum_{i=1}^n[\frac{\partial L(y_i,F(x_i))}{\partial F(x_i)}])$

**ì´ ë•Œ, ìŒì˜ ê·¸ë¼ë””ì–¸íŠ¸($-[\frac{\partial L(y_i,F(x_i))}{\partial F(x_i)}]$)ë¥¼ $z_i$ë¼ê³  ì •ì˜í•œë‹¤ìŒ $\gamma_m$ì— ëŒ€ì…í•˜ë©´ `ì•Œê³ ë¦¬ì¦˜ 2ë²ˆ`**

> $\gamma_m = \arg \min_{\gamma >0}\sum_{i=1}^n L(y_i, F_{m-1} - \gamma z_i)... \ Eq(4)$

<br>

## 3ï¸âƒ£ ì•Œê³ ë¦¬ì¦˜ 3ë²ˆ. $z_i$ ì¦‰ residualì„ targetìœ¼ë¡œ ëª¨ë¸ $g(x)$ì í•©

residualì— ëŒ€í•œ ì‹ **$z_i$**ì„ outputìœ¼ë¡œ, **$x_i$**ë¥¼ inputìœ¼ë¡œ í•˜ëŠ” ìƒˆë¡œìš´ í•¨ìˆ˜ ëª¨ë¸ $g(x)$ë¥¼ í•™ìŠµí•©ë‹ˆë‹¤.

ì• ë‹¨ê³„ì—ì„œ êµ¬í•œ ì”ì°¨ë¥¼ ì˜ˆì¸¡í•˜ëŠ” `base learner`ë¥¼ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.

<br>

## 4ï¸âƒ£ ì•Œê³ ë¦¬ì¦˜ 4ë²ˆ. ê²½ì‚¬í•˜ê°•ë²•ì˜ ë‹¨ê³„ í¬ê¸° ì„ íƒ. $\gamma = \arg \min_{\gamma} \sum^N_{i=1} \Psi(y_i,\hat{f}(x_i) + \gamma g(x_i))$

ì•ì„œ ë§Œë“¤ì—ˆë˜ base learner $g(x)$ëŠ” ì”ì°¨ $z_i$ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤.

ì´ë¥¼ `Eq.(4)`ì— ì´ìš©í•˜ë©´?

> $\gamma_m = \arg \min_{\gamma>0} \sum^N_{i=1} L(y_i,F_{m-1} + \gamma g_m(x_i))$

<br>

## 5ï¸âƒ£ ì•Œê³ ë¦¬ì¦˜ 5ë²ˆ. ì—…ë°ì´íŠ¸ $\hat{f}(x) \leftarrow \hat{f}(x) + \gamma g(x)$

ì• ë‹¨ê³„ì—ì„œ ë³´ì•˜ë˜ ì‹ë“¤ê³¼ ì—°êµ¬ìê°€ ì •í•œ **í•™ìŠµë¥  $l$ì„ ì´ìš©í•˜ì—¬ ëª¨í˜•ì„ ë‹¤ìŒê³¼ ê°™ì´ ì—…ë°ì´íŠ¸ í•©ë‹ˆë‹¤**

> $F_m(x) = F_{m-1}(x) + l \times\gamma_m \times g_m(x)$

í”Œì–´ì„œ ì¨ë³´ë©´ $\approx F_m = F_{m-1} -l\gamma\sum_{i=1}^n[\frac{\partial L(y_i,F(x_i))}{\partial F(x_i)}]$ì´ ë˜ê² ë„¤ìš”.

ë‹¤ì‹œ í•œë²ˆ ì •ë¦¬í•´ë³´ë©´

- $l$ : ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ shrinkage(í•™ìŠµë¥ )
  
- $\gamma$ : ê²½ì‚¬í•˜ê°•ë²•ì˜ stepwise size
  
- $g(x)$ : ì…ë ¥ë²¡í„° $x_i$ë¥¼ ë°›ì•„ ì”ì°¨ì¸ $z_i$ë¥¼ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜
  

# âš“ ë§ˆì¹˜ë©°

---

ì•Œê³ ë¦¬ì¦˜ ìœ ë„ë¥¼ ì´í•´í•˜ê¸° ì¢€ ì–´ë µê¸´ í–ˆëŠ”ë°.. ë©°ì¹ ë™ì•ˆ ë¶™ì¡ê³  ìˆìœ¼ë‹ˆê¹Œ ì¡°ê¸ˆì”© ì´í•´ë˜ë”ë¼êµ¬ìš”? ìœ íŠœë¸Œë„ ì¢€ ë„ì›€ì´ ëê³ .. ì•„ ê·¸ë¦¬ê³  `chat GPT`ëŠ” ë³„ë¡œì¸ê²ƒ ê°™ì•„ìš”. ì•„ë¬´ë˜ë„ í…ìŠ¤íŠ¸ë¥¼ ì˜ˆì¸¡í•´ì„œ ë§Œë“¤ì–´ì£¼ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ì§€ ìˆ˜ì‹, ì—°ì‚° ë“± ê³ ë“±ê¸°ëŠ¥ì€ ì˜ ëª»í•˜ë”ë¼êµ¬ìš” ã…œã…œ

**`ì´ìƒ gbm ì•Œê³ ë¦¬ì¦˜ ë¶„ì„ì´ì—ˆìŠµë‹ˆë‹¤! â˜ ï¸`**

> ì°¸ì¡°
> 
> [Gradient boosting - Wikipedia](https://en.wikipedia.org/wiki/Gradient_boosting)
>
> ì¤‘ì•™ëŒ€í•™êµ ê°•ì˜
