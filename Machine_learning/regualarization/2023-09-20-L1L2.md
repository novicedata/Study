---
excerpt: "ë¦¿ì§€, ë¼ì˜ì— ëŒ€í•´ ì•Œì•„ë³´ì"

categories:
  - Regularization

tags:
  - [í†µê³„ ë¶„ì„]

toc: true
toc_sticky: true

breadcrumb: true

date: 2023-09-20
last_modified_at: 2023-09-20

title: "ì •ê·œí™” : ë¦¿ì§€(Ridge), ë¼ì˜(Lasso) with R"
---
<br>

# ğŸ“Œ Ridge regression(ë¦¿ì§€ íšŒê·€)
---

**`ë¦¿ì§€`**ëŠ” í•˜ë‚˜ì˜ ì •ê·œí™” ë°©ë²•ìœ¼ë¡œ ë³€ìˆ˜ë“¤ ì‚¬ì´ì˜ ê³µë³€ëŸ‰ì„ ì¡°ì •í•´ì¤ë‹ˆë‹¤. í•œë²ˆ ì•„ë˜ì˜ ì„¤ëª…ê³¼ í•¨ê»˜ ë´…ì‹œë‹¤.

ì•„ë˜ì™€ ê°™ì€ linear regression(ì„ í˜• íšŒê·€)ê°€ ìˆìŠµë‹ˆë‹¤.

$$
y_i = \beta_0 + \sum_{j=1}^p \beta_j x_{ij} + \epsilon_i
$$

ì—¬ê¸°ì„œ ë³¼ê±´ $x$ê°€ 2ì°¨ì›ì´ë¼ëŠ” ê²ƒì…ë‹ˆë‹¤. 1ì°¨ì› ë²¡í„°ê°€ ì•„ë‹Œ í–‰ë ¬ë¡œ ë‚˜íƒ€ë‚  ê²ƒì…ë‹ˆë‹¤.

ì´ì— ë”°ë¥¸ **`least squres solution`**ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

$$
\hat{\beta}_S = (X^TX)^{-1}X^TY
$$

`ì™œ ì´ëŸ° ì‹ì´ ë‚˜ì˜¤ëŠ”ì§€ëŠ” ì•„ë˜ ë¸”ë¡œê·¸ë¥¼ ì°¸ê³ í•´ì£¼ì„¸ìš”. ìì„¸íˆ ë‚˜ì™€ìˆì–´ ë³´ê¸° ì¢‹ìŠµë‹ˆë‹¤.`

- *[ìµœì†Œì œê³±ë²•(Least Square Method) - ì¼ë°˜ì ì¸ ë°©ë²• - ì˜êµ¬ë…¸íŠ¸](https://satlab.tistory.com/10)*
  

ìì—¬ê¸°ì„œ $\hat{\beta_S}$ì˜ ê³µë³€ëŸ‰ì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤.

$$
Cov(\hat{\beta_S}) = \sigma^2(X^TX)^{-1}
$$

**`ë¬¸ì œëŠ”`** ë§Œì•½ predictorë“¤ì´ ì„ í˜• ì¢…ì†ì´ë¼ë©´ $X^TX$ê°€ íŠ¹ì´í–‰ë ¬ë¡œ, í‰ê·  ì œê³± ì˜¤ì°¨ê°€ ì»¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ **`ë‹¤ì¤‘ê³µì„ ì„±`**ì´ë¼ê³  í•©ë‹ˆë‹¤.

íŠ¹ì´í–‰ë ¬ì€ í–‰ë ¬ì˜ rankê°€ í–‰ë ¬ì˜ ì°¨ì›ë³´ë‹¤ ì‘ì•„ ì—­í–‰ë ¬ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

## â—¼ï¸ Ridge ì ‘ê·¼

ì´ë¥¼ ìœ„í•´ **`ridge estimator`**ë¥¼ ì´ìš©í•©ë‹ˆë‹¤.

$$
\hat{\beta_\lambda} = (X^TX + \lambda I)^{-1}X^TY
$$

ë³´ì‹œë©´ ì›ë˜ `least squres solution`ì— $\lambda I$ê°€ ì¶”ê°€ëìŠµë‹ˆë‹¤. ì´ estimatorëŠ” íŒ¨ë„í‹°ê°€ ë¶€ì—¬ëœ ì œê³± ì˜¤ì°¨ì˜ í•©ì„ ìµœì†Œí™”í•©ë‹ˆë‹¤. ì•„ë˜ ì‹ì„ í•œë²ˆ ë´…ì‹œë‹¤.

$$
\sum_{i=1}^n(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \lambda\sum_{j=1}^p \beta_j^2
$$

- $\lambda$ : shrinkage parameter. ìš°ë¦¬ê°€ ì •í•´ì¤„ ìˆ˜ ìˆëŠ” íŒŒë¼ë¯¸í„° ì…ë‹ˆë‹¤.
  
- ë§Œì•½ $\lambda=0$ìœ¼ë¡œ ê°„ë‹¤ë©´?? **`ridge estimator = least squreas estimator`**ê°€ ë©ë‹ˆë‹¤. íŒ¨ë„í‹° í•­ì¸ $\lambda\sum_{j=1}^p \beta_j^2 =0$ì´ ë˜ë‹ˆê¹Œìš”.
  
- ë§Œì•½ $\lambda \rightarrow \infty$ë¡œ ê°„ë‹¤ë©´?? ì „ì²´ì‹ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ì„œëŠ” $\beta_j^2$ê°’ì´ 0ì— ê°€ê¹Œì›Œì•¼ í•©ë‹ˆë‹¤.($\hat{\beta}_{Ridge} \rightarrow 0$). ì¦‰ íšŒê·€í–ˆì„ ì‹œ, ë³€ìˆ˜ë“¤ì˜ estimatorê°€ 0ì— ê°€ê¹Œìš´ ê²ƒì´ì£ .
  

### ê²°êµ­?

**ê²°êµ­ ëª©ì ì€ ì´ $\lambda$í•­ì„ ìš°ë¦¬ê°€ ì¡°ì ˆí•˜ì—¬ estimatorë“¤ì„ ì¡°ì •í•¨ìœ¼ë¡œì„œ ë‹¤ì¤‘ê³µì„ ì„±ì˜ ì˜í–¥ì„ ê°ì†Œì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.**

ì „ì— ë¦¿ì§€ íšŒê·€ë¥¼ ì ìš©í•˜ê¸° ì „ì— predictorë“¤ì„ í‘œì¤€í™”í•´ì•¼í•˜ë©°, reponseë˜í•œ ì¤‘ì‹¬í™”ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.(í‰ê·  0ìœ¼ë¡œ)

---

## â—¼ï¸ With R

- Boston dataì´ìš©í•´ medv ì˜ˆì¸¡
  

```r
# Boston dataì˜ medvì˜ˆì¸¡ ìœ„í•´ x, yë‚˜ëˆ„ê¸°
library(MASS)
x = model.matrix(medv~., Boston)[,-1]
y = Boston$medv
```

- glmnet ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ìš©í•´ì„œ ë¦¿ì§€ íšŒê·€
  

```r
# glmnet í†µí•´ì„œ ë¦¿ì§€ íšŒê·€
library(glmnet)
grid = 10^seq(10,-2, length=100)  # ëŒë‹¤ ë§Œë“¤ì–´ì£¼ê¸°
ridge = glmnet(x,y,alpha=0, lambda=grid) # alpha=0ì¼ ë•Œ ë¦¿ì§€

# ëŒë‹¤ê°’ í™•ì¸
str(grid)
>>  num [1:100] 1.00e+10 7.56e+09 5.72e+09 4.33e+09 3.27e+09
```

str(grid) ê°’ì„ ë³´ë©´ 100ê°œì˜ lambdaê°€ ìˆê³ , ìƒìœ„ 5ê°œì˜ ê°’ì„ ë´…ë‹ˆë‹¤.

- ê²°ê³¼ í™•ì¸
  

```r
# ê²°ê³¼ë¥¼ ë³´ë©´ ê° feature(beta0~beta13), ê°ê°ì˜ ëŒë‹¤ ê°¯ìˆ˜ 100ê°œ
dim(coef(ridge))
>> 14 100
```

```r
# ëŒë‹¤ ë³„ coefê°’
print(coef(ridge))
```

![image](https://github.com/novicedata/scrap-comment/assets/88019539/27e9cc04-3699-481d-bf7d-35d441e8f1d8){: .align-center}

ì´ëŸ¬í•œ ê°’ë“¤ì´ 100ê°œê°€ ë‚˜ì˜¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

```r
# í•œë²ˆ 42ë²ˆì§¸ ëŒë‹¤ì¼ ë•Œ ê°’ì„ ë³´ì.
predict(ridge, s=42, type='coefficient')
```

![image](https://github.com/novicedata/scrap-comment/assets/88019539/8982f59d-528c-4188-922f-6c56df9cf660){: .align-center}

42ë²ˆì§¸ ëŒë‹¤ê°’ì¼ ë•Œ ê° ë³€ìˆ˜ì˜ coefê°’ì…ë‹ˆë‹¤.

### 10-foldë¡œ í•´ë³´ê¸°

---

```r
# ë°ì´í„° ìŠ¤í”Œë¦¿ 8:2
library(caTools)
x = model.matrix(medv ~ ., data = Boston)[,-1]  # interceptë¥¼ ì œì™¸í•œ í–‰ë ¬
y = Boston$medv

set.seed(42)

split = sample.split(y, SplitRatio = 0.8)
train = which(split==TRUE)
test = which(split==FALSE)

x_train = x[train,]
x_test = x[test,]
y_train = y[train]
y_test = y[test]

# 10-fold cv(defaultê°€ 10fold)
cv = cv.glmnet(x_train, y_train, alpha=0)
plot(cv)
```

![image](https://github.com/novicedata/template_ML/assets/88019539/942b5746-6e5c-4bf8-9a1b-ed3c4dff51c3){: .align-center}

ê·¸ë˜í”„ ë³´ì‹œë©´ 0ê³¼2ì‚¬ì´ì—ì„œ 2ì— ê°€ê¹Œìš¸ë•Œê°€ bestì¸ ê²ƒ ê°™ì€ë° best lambda ê°’ì„ í•œë²ˆ ë´…ì‹œë‹¤.

```r
# best lambdaë³´ê¸°
best = cv$lambda.min
best
>> 0.7010396
```

best ëŒë‹¤ë¥¼ ë³´ì•˜ìœ¼ë‹ˆ ë§ˆì§€ë§‰ìœ¼ë¡œ ì „ì²´ ë°ì´í„°ì— ì ìš©í•´ë´…ì‹œë‹¤.

```r
# best lambdaë¡œ cvê°€ ì•„ë‹Œ ì „ì²´ ë°ì´í„°ì— í•´ë³´ì
final = glmnet(x_train, y_train, alpha=0, lambda=best)

# pred í•˜, mean square error êµ¬í•˜ê¸°
pred = predict(final, x_test)
mean((y_test-pred)^2)
>> 25.66486
```

<br>

# ğŸ“Œ Lasso regression(ë¼ì˜ íšŒê·€)

---

**`LASSO`**ê°€ **`Least Absolute Shrinkage and Selection Operator`**ì…ë‹ˆë‹¤. ì¤‘ê°„ì— Selectionì´ ìˆì£ ?? ë¦¿ì§€ì™€ëŠ” ë‹¤ë¥´ê²Œ ê³„ìˆ˜($\beta$)ê°€ 0ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰, ë¦¿ì§€ì™€ëŠ” ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ê³„ìˆ˜ë¥¼ 0ìœ¼ë¡œ ë§Œë“¤ì–´ ì •ë§ ë‹¤ì¤‘ê³µì„ ì„±ì— ê±¸ë¦¬ì§€ ì•Šê³  ì›í•˜ëŠ” interceptë§Œ ì–»ì„ ìˆ˜ ìˆëŠ” ê²ƒ ì…ë‹ˆë‹¤.

## â—¼ï¸ Lasso ì ‘ê·¼

Ridgeì™€ ë‹¤ë¥¸ `LASSO solution`ì„ ë³´ê² ìŠµë‹ˆë‹¤. ì•„ë˜ë¥¼ ìµœì†Œí™” í•´ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤.

$$
\sum_{i=1}^n(y_i - \beta_0 - \sum_{j=1}^p \beta_j x_{ij})^2 + \lambda \sum_{j=1}^p \|\beta_j \|
$$

Lidgeì‹ê³¼ëŠ” ë‹¤ë¥´ê²Œ ë§ˆì§€ë§‰ í˜ë„í‹° í•­ì—ì„œ **ì œê³±ì´ ì•„ë‹Œ ì ˆëŒ“ê°’ì„ êµ¬í•´ì¤ë‹ˆë‹¤.**

- ë§Œì•½ $\lambda=0$ìœ¼ë¡œ ê°„ë‹¤ë©´?? **`lasso estimator = least squreas estimator`**ê°€ ë©ë‹ˆë‹¤. ì´ê±´ ë¦¿ì§€ì™€ ê°™ì€ì´ìœ ì£ .
  
- ë§Œì•½ $\lambda \rightarrow \infty$ë¡œ ê°„ë‹¤ë©´?? ì „ì²´ì‹ì„ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ì„œëŠ” $\|\beta_j\|$ê°’ì´ 0ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
  

**`ì™œ`** LassoëŠ” ê³„ìˆ˜ê°€ 0ì´ ë˜ëŠ”ë° LidgeëŠ” ë˜ì§€ ì•ŠëŠ”ì§€ëŠ” ì•„ë˜ ë¸”ë¡œê·¸ì— ì˜ ì •ë¦¬ë˜ì–´ ìˆëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.

- *[Ridge regression(ë¦¿ì§€ íšŒê·€)ì™€ Lasso regression(ë¼ì˜ íšŒê·€) ì‰½ê²Œ ì´í•´í•˜ê¸°](https://rk1993.tistory.com/entry/Ridge-regression%EC%99%80-Lasso-regression-%EC%89%BD%EA%B2%8C-%EC%9D%B4%ED%95%B4%ED%95%98%EA%B8%B0){: .align-center}*

<br>

## â—¼ï¸ With R

---

- Boston dataì´ìš©í•´ medv ì˜ˆì¸¡

```r
# Boston dataì˜ medvì˜ˆì¸¡ ìœ„í•´ x, yë‚˜ëˆ„ê¸°
library(MASS)
x = model.matrix(medv~., Boston)[,-1]
y = Boston$medv
```

- glmnet ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ìš©í•´ì„œ ë¦¿ì§€ íšŒê·€

```r
library(glmnet)
grid = 10^seq(10,-2,length=100) # ë³´í†µ lambdaë¥¼ ì´ë ‡ê²Œ ë„£ì–´ì¤€ë‹¤ê³  
lasso.mod = glmnet(x,y,alpha=1,lambda=grid) # alpha 1ì¼ë•Œ ë¼ì˜

# ëŒë‹¤ê°’ í™•ì¸
str(grid)
>>  num [1:100] 1.00e+10 7.56e+09 5.72e+09 4.33e+09 3.27e+09
```

str(grid) ê°’ì„ ë³´ë©´ 100ê°œì˜ lambdaê°€ ìˆê³ , ìƒìœ„ 5ê°œì˜ ê°’ì„ ë´…ë‹ˆë‹¤.

- ê²°ê³¼ í™•ì¸

```r
# ê²°ê³¼ë¥¼ ë³´ë©´ ê° feature(beta0~beta13), ê°ê°ì˜ ëŒë‹¤ ê°¯ìˆ˜ 100ê°œ
dim(coef(lasso))
>> 14 100
```

```r
# ëŒë‹¤ ë³„ coefê°’
print(coef(lasso))
```

![image](https://github.com/novicedata/template_ML/assets/88019539/6027dfce-222d-43a5-9c47-062d4835d1a9){: .align-center}

### coef ì°¨ì´

**ìœ„ lasso coefë¥¼ ë³´ì‹œë©´ interceptë¥¼ ì œì™¸í•œ ê°’ë“¤ì´ ëª¨ë‘ 0ì…ë‹ˆë‹¤. ì´ê²Œ ridgeì™€ lassoì˜ ì°¨ì´ì…ë‹ˆë‹¤. ëŒë‹¤ê°’ì´ ë§¤ìš° í´ìˆ˜ë¡ ê° ë² íƒ€ì˜ ê³„ìˆ˜ë“¤ì´ ê±°ì˜ 0ì´ ë©ë‹ˆë‹¤.**

### 10-foldë¡œ í•´ë³´ê¸°

---

```r
# ë°ì´í„° ìŠ¤í”Œë¦¿ 8:2
library(caTools)
x = model.matrix(medv ~ ., data = Boston)[,-1]  # interceptë¥¼ ì œì™¸í•œ í–‰ë ¬
y = Boston$medv

set.seed(42)

split = sample.split(y, SplitRatio = 0.8)
train = which(split==TRUE)
test = which(split==FALSE)

x_train = x[train,]
x_test = x[test,]
y_train = y[train]
y_test = y[test]

# 10-fold cv(defaultê°€ 10fold)
cv = cv.glmnet(x_train, y_train, alpha=1)
plot(cv)
```

![image](https://github.com/novicedata/template_ML/assets/88019539/a9eb0270-caf8-4371-a3ab-f077e859fca4){: .align-center}

ë˜‘ê°™ì´ best lambdaë¥¼ êµ¬í•˜ê³  predict í•´ë´…ì‹œë‹¤.

```r
# best lambdaë³´ê¸°
best = cv$lambda.min
best 

# ì „ì²´ ë°ì´í„°ê°€ì§€ê³  fit
final = glmnet(x_train,y_train, alpha=1, lambda=best)

# pred
pred=predict(final, x_test)
mean((y_test-pred)^2)
>> 24.54536
```

ë¦¿ì§€ë¥¼ í–ˆì„ ë•Œ ë³´ë‹¤ ì¡°ê¸ˆ ë” ì‘ê²Œ ë‚˜ì™”ìŠµë‹ˆë‹¤.

### lambda ê°’ì— ë”°ë¥¸ coef 0ì˜ ê°œìˆ˜ ë³€í™”

---

```r
# traced ê·¸ë¦¼. log lambdaê°’ì— ë”°ë¼ì„œ ëª‡ê°œê°€ 0ì´ ì•„ë‹Œì§€ ê·¸ë˜í”„ ìœ„ ìˆ«ìì—ì„œ ë³´ì—¬ì¤Œ.
plot(lasso, xvar='lambda', label=TRUE)
```

![image](https://github.com/novicedata/template_ML/assets/88019539/bfdcbce2-8484-4334-ab09-77d3cfa71299){: .align-center}

ê·¸ë¦¼ì„ ë³´ì‹œë©´ ê·¸ë˜í”„ ìœ„ ì¶•ì€**coef=0ì´ ì•„ë‹Œ ê²ƒì˜ ê°œìˆ˜ì…ë‹ˆë‹¤.** **log lambdaê°’ì´ ì»¤ì§ˆ ìˆ˜ë¡ ëŒ€ë¶€ë¶„ì˜ coefê°’ì´ 0ì´ ë˜ëŠ” ê²ƒì„ ë³¼ìˆ˜ ìˆìŠµë‹ˆë‹¤.**
