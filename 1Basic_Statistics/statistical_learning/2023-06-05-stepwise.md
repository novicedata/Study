---
excerpt: "선형 모델의 확장을 알아보자."

categories :
  - Statistical learning
tags :
  - [통계 학습]

toc: true
toc_sticky: true

date: 2023-06-05
last_modified_at: 2023-06-05

title: "전진 단계적 선택(Forawrd stepwise), 후진 단계적 선택(Backward stepwise)"
---

우리는 앞서 선형모델을 최소제곱법을 사용하여 추정치를 찾았습니다.
위 최소제곱법뿐 아니라 좀 더 예측에 대한 정확도가 높거나, 모델이 해석하는 능력을 높이는데 사용되는 다른 적합절차를 알아 볼 것입니다.
<br>
# 선형모델의 확장 : 부분집합 선택법
---
우선 부분집합 선택법을 간단히 알아봅시다.

선형모델의 적합절차를 수행하기 위해 p개인 설명변수의 가능한 조합 각각에 대해서 최소제곱회귀를 적합합니다.


쉽게 설명한다면 

**단 하나의 설명변수를 포함하는 모델 p개**
**두 개의 설명변수를 포함하는 모델 p(p-1)/2 개 ...**

이런 식으로 모든 모델들을 적합합니다. 그 다음 이 모델들중에 가장 좋은 모델을 찾아내는 것이죠.

예시로 설명해봅시다.

설면변수가 3개 있다면 이를 A, B, C로 놓아 봅시다. 이에 대해 가능한 모델 조합은

**y = x**

**y = Ax / y = Bx / y = Cx**

**y = Ax_1 + Bx_2 / y = Ax_1 + Cx_2 / y = Bx_1 + Cx_2**

**y = Ax_1 + Bx_2 + Cx_3**

이렇게 될 것입니다.

**예시에 나온 총 8개의 모델 중 최고의 모델을 선택하는 것입니다.**
<br>
# RSS와 R^2
---
**모델에 포함된 변수가 많을 수록 모델의 예측 오차는 감소할 수 있습니다.** 

**즉 RSS**
: 잔차제곱합(모델이 예측한 값, 실제 값의 차이를 제곱하여 더한 것 : 클수록 모델의 예측력이 안좋다는 것)은 변수를 추가 할수록 감소한다는 것입니다.

변수가 증가 할수록 설명하기 힘들던 데이터의 특성들을 추가로 설명해줄 수 있기 때문에 예측력은 좋아질 수밖에 없는 것입니다.

**반대로 모델에 포함된 변수가 많을 수록 모델의 해석력은 증가할 수 있습니다.**

**즉 R^2** 
: 조정된 R값(모델의 적합도. 클수록 모델이 종속 변수의 변동성을 더 잘 설명한다는 것.)은 변수를 추가 할수록 증가한다는 것입니다.

변수가 증가 할수록 모델은 더 많은 정보를 종속 변수의 변동성 설명에 활용할 수 있게되고, 추가된 변수가 종속변수와 관련이 있는 것이라면, 모델은 더 좋은 적합을 할 수 있습니다.

**때문에 우리는 이 사이를 아우르는 적합한 모델을 찾아야 합니다.(작은 RSS, 높은 R^2)**
**위 방법의 문제점은.. 변수의 증가에 따라 모델의 수가 기하급수적으로 늘어난 다는 것입니다.**

위 예시에서 변수는 3개 A,B,C 였고 이에 따른 가능한 모델의 수는 8(2의 3제곱)이었습니다. 그렇다면 변수를 10개만 하여도 1024(2의 10제곱)개입니다.

<br>

# 전진 단계적 선택(forward stepwise selection), 후진 단계적 선택(backward stepwise selection)
---
위 단점을 완화하기 위해 우리는 단계적 선택을 할것입니다.
<br>
## 1. 전진 단계적 선택

**이는 우선 설명 변수가 아에 포함되지 않은 모델부터 시작해서 모든 설명변수가 포함될 때 까지 전진적으로 하나씩 설명 변수를 추가합니다.** 

이 때, *단계적으로 모델에 추가되는 변수는 모델 성능 향상에 가장 큰 영향을 주는 변수로 선택*됩니다.

예를 들어 봅시다. **우리가 4개의 변수 A, B, C, D 중 최대 2개의 변수를 사용하여 최고의 모델을 찾으려고 합니다.** 이 때 전진 단계적 선택법을 사용한다면,

**y = x**

**y = Ax / y = Bx / y = Cx / y = Dx**

**y = Ax_1 + Bx_2 / y = Ax_1 + Cx_2 / y = Ax_1 + Dx_2 / y = Bx_1 + Cx_2 / y = Bx_1 + Dx_2 / y = Cx_1 + Dx_2**

의 모델들을 적합해서 최고의 모델을 찾을 것입니다.

우선 **장점은 맨 처음 설명한 부분집합 선택은 2^p개의 모델을 확인해야 하고 전진 단게적 선택법은 (p(p+1)/2) + 1개의 모델만 확인하면 됩니다.**

그렇지만 **단점 또한 존재합니다. 위의 예시를 들어 설명해보죠.**
**만약 3개의 변수로 설명하는 모델이 최고의 모델이었다면?? 우리는 최대 2개의 변수를 통한 모델이 가장 좋은 성능을 가졌다고 보았기 때문에 모든 변수를 보는 기법보다 못한 결과를 얻는 것이죠.**

전진 선택적 방법에서 만약 1개의 변수를 사용했을 때 y = Bx가 가장 좋은 성능을 보였다면, 그 후 2개의 변수를 사용할 때는 y = Bx_1 + Ax_2 y = Bx_1 + Cx_2 / y = Bx_1 + Dx_2 만을 확인합니다. 
만약 y = Ax_1 + Cx_2 가 더 좋은 모델이었다면? 1개의 변수를 사용했을 때 y = Bx가 가장 좋았기 때문에 2개의 변수를 사용할 때 다른 경우의 수는 확인 할 수 없습니다. y = Ax보다 y = Bx가 성능이 더 좋았기 때문의 A를 포함한 2개의 변수 모델을 확인하지 못한거죠.

## 2. 후진 단계적 선택

전진 단계적 선택과 메커니즘은 비슷하지만, **반대로 모든 설명변수가 포함된 모델부터 단계적으로 설명력이 제일 낮은 설명변수를 제외하며 모델을 찾아나갑니다.**

**같은 방법이지만 방향만 다른 것입니다.**

**장, 단점 모두 비슷하겠죠?**

전진과 후진을 합한 하이브리드 방식도 있습니다.
간단히 설명한다면 새로운 변수를 전진적으로 추가해 나가는 단계에서 모델 성능을 더 이상 향상시키지 않은 변수는 제거해 나가는 방법입니다.

