베이지안 통계입니다.

아래 간단한 설명을 한 글이 있습니다.

[https://datanovice.tistory.com/entry/%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%EC%9D%B4%EB%A1%A0vs-%EB%B9%88%EB%8F%84%EC%A3%BC%EC%9D%98](https://datanovice.tistory.com/entry/%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%EC%9D%B4%EB%A1%A0vs-%EB%B9%88%EB%8F%84%EC%A3%BC%EC%9D%98)

 [베이지안 이론(vs 빈도주의)

베이지안 이론은 머신러닝에 있어서 아주 중요합니다. 보통 ML에서 쓰이는 데이터는 일반 확률론으로는 한계가 있고 ML자체가 특정 가성의 확률을 높이는 최적화된 모델을 찾는 것을 목적으로

datanovice.tistory.com](https://datanovice.tistory.com/entry/%EB%B2%A0%EC%9D%B4%EC%A7%80%EC%95%88-%EC%9D%B4%EB%A1%A0vs-%EB%B9%88%EB%8F%84%EC%A3%BC%EC%9D%98)

## 📌 베이지안 정리

우리가 알고싶은, 관심있는 파라미터를 $\\theta$라고 해봅시다.

**빈도주의의 경우 이 $\\theta$를 알려지지 않은 상수라고 봅니다.**

**반면에 베이지안은 $\\theta$를 확률 변수(자기만의 분포가 있는)라고 봅니다.**

이는 분명 차이가 있습니다. **빈도주의의 경우 어떠한 정해져있는, 개념적으로 움직이지 않는 어떠한 값이라고 볼 수 있지만, 베이지안은 정해져있지 않고 특정한 분포속에 존재하는 값이라고 보는 것입니다.**

이 둘이 다른 것 처럼 표기하는 법에도 조금은 차이를 보입니다.

빈도주의의 표기가 아래와 같다면

$$  
f(x\_1,...,x\_n)  
\\\\ X \\sim N(\\mu,1)  
$$

베이지안의 표기는 아래와 같습니다.

$$  
f(x\_1,...,x\_n|\\theta)  
\\\\ X|\\mu \\sim N(\\mu,1)  
$$

항상 어떠한 조건이 붙는걸 볼 수 있습니다.

**베이지안 분석은 사전 정보인 $\\pi(\\theta)$와 표본 정보인 $(y, P(y|\\theta))$ 를 결합하여 주어진 $y$에 대한 $\\theta$의 사후 확률밀도 함수(posterior PDF) 함수를 만드는 것을 의미합니다.**

쉽게 풀어보자면 $\\theta$에 대한 어떤 사전정보와 표본의 정보(우도함수)를 결합하여 $y$ 에 대한 $\\theta$의 다음 확률밀도 함수를 구하는 것입니다.

-   사전 정보 : $\\pi(\\theta)$
-   표본 정보(우도함수) : $(y, P(y|\\theta))$

$y$가 나와서 당황스럽긴 하지만, 베이지안 통계에서는 일반 통계의 $x$와 같은 느낌입니다.

어쨋든 이렇게 구할 **사후 확률밀도 함수**는 아래처럼 표시합니다.

$$  
h(y, \\theta) = P(y|\\theta)\\pi(\\theta) \\rightarrow P(\\theta|y)  
$$

**이때 $Y$는 파라미터 $\\theta$를 특정값으로 고려하지 않고 데이터 $Y$ 자신 만을 고려할 때의 주변 확률 밀도함수를 같습니다. 아래와 같습니다.**

$$  
m(y) = \\int h(y, \\theta) d\\theta  
$$

식을 보면 $\\theta$에 대해 적분함으로써 자기자신의 함수를 가짐을 볼 수 있습니다.

**위에서 보았던 $P(\\theta|y)$의 경우 데이터 $Y$가 주어진 상황에서 파라미터 $\\theta$가 어떻게 분포하는지 나타낸 조건부 확률로 아래와 같습니다.**

$$  
P(\\theta|y) = \\dfrac{h(y,\\theta)}{m(y)}  
$$

이식에서 위에서 보았던 $h(y, \\theta)$(데이터 $Y$가 파라미터 $\\theta$에 의해 생성되는 확률)를 대입한다면 아래와 같습니다.

$$  
P(\\theta|y) = \\dfrac{P(y|\\theta)\\pi(\\theta)}{m(y)}  
$$

자 이를 풀어써보면

**$y$일때 $\\theta$의 사후확률 = ($\\theta$일 때 $y$가 관찰될 확률을 나타낸 우도함수 \* 사전 정보(사전 확률분포))/데이터 정보****입니다. 이렇게 사후확률은 우도함수와 사전정보를 joint한 것으로 나타내며 이 $P(\\theta|y)$를 사후 확률밀도 함수 라고 합니다.**

## 📌 사후 확률밀도함수 비례식

$P(\\theta|y)$에 대한 비례식은 아래와 같으며 이를 통해 베이즈 모델 형식을 구성합니다.. **그렇다면 왜 굳이 비례식을 사용할까요?**

$$  
P(\\theta|y) \\propto h(y, \\theta) = P(y|\\theta)\\pi(\\theta)  
\\\\ P(\\theta|y) \\propto P(y|\\theta)\\pi(\\theta)  
$$

사후 = (가능도 \* 사전)/데이터 _(간단하게씀)_ 을 볼 때 이미 데이터는 어떠한 상수값입니다. 애초에 우리가 관심있는건 어떠한 데이터가 아니라 바로 $\\theta$값입니다. 때문에 **이러한 상수값은 $\\theta$를 구하는데 아무런 영향을 끼치지 못하기 때문에 생략하여 비례식을 사용하는 것입니다.**

## 📌 베이지안 추론

**위에서 보았던 식과 같이 베이지안 추론은 posterior pdf인 $P(\\theta|y)$가 기반이 되어 시작됩니다.**

$$  
P(\\theta|y) \\propto P(y|\\theta)\\pi(\\theta)  
$$

베이지안 추론에서는 결국 이 $\\pi(\\theta)$인 사전 확률밀도함수가 굉장히 중요합니다.

우리가 예측할(추론할) 미래의 관측치 $\\tilde{y}$가 있다고 해봅시다.(설령 내일의 날씨라던가) 이때 주어진 데이터 $y$에 대한 $\\tilde{y}$의 사후 확률 밀도 함수는 아래와 같을 겁니다.

$$  
P(\\tilde{y}|y)  
$$

조건부분포에 따라 아래와 같이 정리할 수 있습니다.

$$  
\\begin{align}  
P(\\tilde{y}|y) $= \\dfrac{P(\\tilde{y}, y)}{P(y)} = \\dfrac{\\int P(\\tilde{y}, y, \\theta)d\\theta}{P(y)}  
\\\\ $= \\dfrac{\\int P(\\tilde{y}|y,\\theta)P(y,\\theta)d\\theta}{P(y)}  
\\end{align}  
$$

이 때 $\\theta$ 가 주어진 조건에서 $\\tilde{y}, y$가 서로 독립이라면 아래와 같이 표기할 수 있습니다.

$$  
P(\\tilde{y}|y) = \\int P(\\tilde{y}|\\theta)P(\\theta|y) d \\theta  
$$

**이 분포를 활용하여 우리는 관측하지 않은 미래의 혹은 새로운 데이터에 대한 확률분포를 추정할 수 있습니다.**
