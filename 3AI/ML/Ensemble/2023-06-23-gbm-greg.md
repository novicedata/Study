---
excerpt: "R의 gbm vignette 리뷰"

categories:
  - Machine learning

tags:
  - [Boost]

toc: true
toc_sticky: true

breadcrumb: true

date: 2023-06-23
last_modified_at: 2023-06-23

title: "[Machine learning, 머신러닝] GBM(Greg Ridgeway. 2020) 리뷰"
---

<br>

# Generalized Boosted Models : A guide to the gbm package(Greg Ridgeway. 2020) 리뷰

---

<br>

해당 문서는 R Cran의 gbm 참고 파일로 gradient boosting의 전반적인 알고리즘과 공식유도 그리고 이점을 다루고 있습니다.

<br>

## **⚙️ Friedman's gradient boosting machine**

프리드먼이 제시한 **`gradient boost algorithm`** 에 대한 설명입니다. 해당 공식의 유도는 후에 자세한 포스팅을 추가로 올릴 예정입니다!

**💻 알고리즘 설명.**

> **$\hat{f}(x) to be constant$ : $\hat{f}(x) = \arg\min_p\sum_{i=1}^N\Psi(y_i,\rho).$**
> 
> **`맨 처음. 실제값과의 손실함수를 최소화 하는 상수를 찾습니다.`**
> 
> ***for* $t$ in $1,...,T$**
> 
> > ***Step 1.*** working response로 음의 그레디언트를 계산
> > 
> > **`해당 식은 실제값과 예측값의 손실함수를 미분한 값에 음수를 취한 값으로 -(gradient)이자 residual(잔차)입니다.`**
> > $$Eq.(1) :\ z_i = -\frac{\partial}{\partial f(x_i)}\Psi(y_i,f(x_i))|_{f(x_i)=\hat{f}(x_i)}$$
> > 
> > ***Step 2.*** 공변량 $x_i$로 부터 $z_i$를 예측하는 회귀모델 $g(x)$를 적합
> > 
> > **`예측하는데 사용되는 입력값 x로부터 Step.1의 음의 그레디언트 값을 예측하는 함수를 만듭니다.`**
> > 
> > ***Step 3.*** 경사 하강법의 단계 크기 선택 : $Eq.(2) : \rho = \arg \min_{\rho} \sum^N_{i=1} \Psi(y_i,\hat{f}(x_i) + \rho g(x_i))$
> > 
> > **`음의 그레디언트 즉, 음의 기울기값으로 어느 크기만큼 진행할 것인지 위 식의 손실함수를 최소화하는 상수 값을 찾습니다.`**
> > 
> > ***Stpe 4.*** $f(x)$ 업데이트
> > 
> > $Eq.(3) :\hat{f}(x) \leftarrow \hat{f}(x) + \rho g(x)$
> > 
> > **`다음 모델을 위해 업데이트 합니다.`**

이렇게만 보면 굉장히 어렵죠? 코딩은 쉽지만 속의 알고리즘만 보았을 때는 무슨 소리인지 직관적으로 이해가 가지 않습니다.

이에 대한 직관적 설명은 후 포스팅을 참고해주세요.

<br>

## **🔧 부스팅의 성능 향상 위한 방법**

### 1️⃣ learning rate(학습률)

흔히 부스팅은 과적합이 적다고 하지만 저자는 다르다고 생각한다고 합니다. 실제로 많은 부스팅 알고리즘에서 학습률을 도입하여 사용하죠. 이렇듯 저자는 **<U>학습률을 도입하여 제안된 이동을 완화</U>** 할 수 있다고 합니다.

***Step 3.*** 에서 경사 하강법의 단계 크기를 선택하였습니다. 하지만 이 크기가 적절하지 않고 너무 크다면 효율이 떨어집니다.

> 요리를 예로 들어 설명한다면 우리가 간을 맞추기 위해 소금을 넣는다고 해봅시다. 10g정도를 넣어야 간이 알맞아 지는데 실제로 우리가 먹기 전까지는 알 수 없죠.
> 
> 그래서 우리가 15g을 넣는다면? 너무 짜집니다. 다시 짜지지 않게 조치를 취해야합니다. 이를 반복하면 굉장히 비효율적이게 되죠.
> 
> 여기서 학습률을 이용해서 15g에 0.6 언저리를 곱하여 대략 10g을 맞춰주는 겁니다. 여기서 학습률은 0.6 언저리가 되겠죠?

식으로 표현하면 아래와 같습니다.

$\hat{f}(\mathbf{x}) \leftarrow \hat{f}(\mathbf{x}) + \lambda\rho g(x)$

여기서 **$\lambda$가 바로 학습률입니다.**

<br>

### 2️⃣ Variance reduction using subsampling(하위 샘플링)

다음 gradient step을 추정할 때 원래는 x값 모두를 사용합니다. **<U>프리드먼은 데이터셋에서 균일하게 비복원 추출을 하는 확률적 경사 부스틍 알고리즘을 제안</U>**했습니다.

실제로 이 단계가 성능을 크게 향상 시켰다고 하네요.

즉, 모든 데이터를 사용하는게 아니라 데이터 100%중에 랜덤하게 80%를 추출하여 사용하는 것입니다.(80% 예시일 뿐 연구자의 몫입니다.)

<br>

### 3️⃣ ANOVA decomposition(아노바 분해)

일부 함수 근사 방법에는 **<U>기능적 아노바 분해(functional ANOVA decomposition)가</U>** 있습니다.

함수 $f(x)$를 아래과 같이 분해하는 것입니다.

$f(\mathbf{x}) = \sum_jf_j(x_j) +\sum_{jk}f_{jk}(x_j,x_k) + \sum_{jkl}f_{jkl}(x_j,x_k,x_l) ...$

이를 트리 부스팅에도 적용하는 것이 부스팅 모델의 성능을 향상시킬 수 있다고 합니다.

> 트리를 예로 들어 적용하면 one split decision tree(하나의 분기만 갖는 결정 트리)는 하나의 변수에만 의존하며 위 식의 첫 항인 $\sum_jf_j(x_j)$가 될 것이고 두개의 분기만 갖는 결정 트리는 두번째 항인 $\sum_{jk}f_{jk}(x_j,x_k)$이 되고... 이를 반복하는 것입니다.

<br>

### 4️⃣ Relative influence(변수의 상대적 영향력)

프리드먼은 추정치에 대한 변수의 상대적 영향력(relative influence)를 확장하는 방법도 제안했다고 합니다.

트리 기반 방법에서 변수 $x_j$ 의 상대적 영향력은 다음과 같이 표현합니다.![image](https://github.com/novicedata/scrap-comment/assets/88019539/26cc8821-dbbc-4027-a9e4-d616b55ec5c7)




$t$번째 지점에서 $x_j$를 기준으로 분할한 결과 추정에 개선이 일어났다면 1을 출력하고 아니라면 0을 출력합니다. 이들을 모두 더한 값이 $\hat{J}_j^2$이며 변수의 상대적 영향력이 됩니다.

**<U>프리드먼은 이러한 영향력을 부스팅 알고리즘에서 생성된 모든 트리에 대해 평균화해서 나타낼 수 있다고 제안</U>**했습니다. 식으로 표현한다면 $\frac{1}{M}\sum_{splits \ on \ x_j}I_t^2$이고 여기서 $M$은 트리의 전체 개수입니다.

<br>

## **💻 Code, Common user options(R 코드와 간단한 옵션)**

해당 본문에서 R에서의 코드 알고리즘과 옵션에 대한 간단한 설명도 제시해줍니다.

- a loss function **(distribution)** : 손실함수 선택
- the number of iterations, $T$ **(n.trees)** : 트리 개수 선택
- the depth of each tree, $K$ **(interaction.depth)** : 트리 깊이 선택
- the shrinkage (or learning rate) parameter, $\lambda$ **(shrinkage)** : 학습률 선택
- the subsampling rate, $p$ **(bag.fraction)** : 하위 샘플링 비율 선택

코드 알고리즘의 경우 위에서 설명한 알고리즘과 비슷하기 때문에 넘어가겠습니다. 후에 코딩 포스팅을 통해 소개할게요.

<br>

### ◾ Loss function

**<U>확률 분포 즉, 손실함수의 선택입니다.</U>** 많은 분포가 있는데 간단하게 몇개만 소개하자면

- bernoulli : 대부분의 분류 문제에서 베르누이 분포가 권장
  
- gaussian : 연속적인 결과에 대해서는 평균 제곱 오차를 최소화하기 위해 가우시안 분포를 선택
  
- laplace : 절대값 오차를 최소화하기 위해 라플라스 분포 선택
  
- coxph : 생존 분석의 문제라면 콕스 비례 위험 모형 선택
  
- poisson : count 형태의 결과가 주어진다면 포아송 분포 선택
  
- quantile : 결과의 조건부 분포의 백분위수를 추정하기 위해서 분위수 회귀 가능
  

`(이렇게 보니 각 분포에 대해서도 포스팅 해야겠어요. 점점 늘어나는 군요 흐흐)`

<br>

### ◾ Shrinkage and number of iterations(학습률과 트리 개수의 선택)

**<U>gbm에서 대부분 고민하는 문제가 트리의 개수와 학습률의 선택이라고 합니다.</U>**

**`shirnkage(학습률)`**이 작을 수록 거의 예측 성능은 향상되지만, 너무 낮을 경우 그만큼 컴퓨터의 저장공간과 CPU 시간 등의 계산 비용이 발생합니다.

본문에서 일반적으로 shirnkage : 0.01~0.001 그리고 iterations : 3,000 ~ 10,000을 사용한다 합니다.

<br>

### ◾ Optimal number of iterations

위에서 처럼 iterations를 직접 할당해도 되지만 해당 패키지에서 **<U>최적의 값을 위해 세가지 방법을 제공합니다.</U>**

- gbm.perf(...,method='test') : independent test set. 하나의 홀드아웃 테스트 세트를 사용하여 최적 반복횟수를 선택합니다.
  
- gbm.perf(...,method='OOB') : out-of-bag 추정. out-of-bag을 이용하여 추정합니다. 본문에서 이 방법은 항상 너무 보수적인 선택을 한다고 합니다.
  
- gbm.perf(...,method='cv') : v-fold 교차 검증. 우리에게 익숙한 v-fold 방법입니다.
  

실제 본문을 보시면 양이 많습니다. 기본적인 베이스 지식을 요구하기 때문에 저에게는 좀 어려웠는데 그래도 여러 튜토리얼을 보며 이해하려고 노력했네요 ㅜㅜ

다음 포스팅은 알고리즘을 세부적으로 리뷰하고 설명하려고 합니다!

**`이상 gbm in R Cran 리뷰 였습니다! ☠️`**

> 참조
> 
> https://cran.r-project.org/web/packages/gbm/vignettes/gbm.pdf
