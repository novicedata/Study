---
excerpt: "우리가 하는 회귀식은 어떻게 나온걸까?"

categories :
  - Statistical learning

tags :
  - [통계 학습]

toc: true
toc_sticky: true

date: 2023-09-10
last_modified_at: 2023-09-10

title: "손실함수와 회귀식에 대하여"
---

<br>

# 📌 선형회귀에 대하여
---

우선 선형회귀의 경우 **Supervised learning**으로 지도 학습의 일종입니다. 먼저 가정을 해봅시다.

$X$ : input으로 perdictors, covariates, independent variables, features 등으로 불리웁니다. 저같은 경우 feature라고 많이 부르는거 같아요

$Y$ : output으로 reponse variable, dependent variables 등으로 불립니다. 제 전공인 심리학에서는 보통 dependen variables라고 많이 불리우는데 저는 그냥 output으로 부르는게 편하더라구요

$n$ : observations으로 distinct data points의 개수 입니다.

$p$ : variables의 수입니다.

$n, p, X$ 와의 관계를 확인해보면 아래와 같을 겁니다.

![image](https://github.com/novicedata/scrap-comment/assets/88019539/638995b1-ddaa-4921-8b81-5146009ef8a8)

이를 벡터$\textbf{x}$ 로 표현하면 $\textbf{X} = (\textbf{x}_1, ..., \textbf{x}_p)$ 가 되겠네요. 우리가 흔히 보는 **`선형회귀식`**에 대해서 봅시다.

$$
E[Y|X=x] = \beta_0 + \beta_1x + \epsilon
$$

이런 식으로 표현되죠. 그런데 우리는 앞의 E[~] 부분을 그냥 Y로 축약해서 사용합니다.

$$
Y = \beta_0 + \beta_1x + \epsilon
$$

이런 식으로 말이죠. **왜 선형회귀식이 저런 함수로 도출되는지 알아봅시다.**

<br>

# ❓ 왜???
---

우선 앞선 가정을 한번 다시 봅시다. $X$는 $p$차원이고, $Y$의 경우 단일 차원입니다. 이떼 joint probability는 $P(x,y)$가 되겠죠.

우리가 원하는 것은 $f(x) = \hat{Y}$를 찾는 것입니다. 즉, **$X$가 주어질 때 $Y$를 얻을 수 있는 함수를 얻는 것입니다.** 이를 위해선 결국 **`loss function`**이 필요합니다.

가장 흔히 쓰이는 loss function으로는 **`square loss`**가 많이 쓰이죠.

$$
L(a,b) = (a-b)^2
$$

해당 식에서 a,b에 $Y, f(x)=\hat{Y}$ 를 대입해봅시다.

$$
L(Y,f(X)) = (Y-f(X))^2
$$

이는 즉, **실제 $Y$값과 예측된 $\hat{Y}$값의 차이 제곱을 나타내며 이를 최소화해야 합니다.**

여기서 **`모델의 예측 오차는`**

$$
E(Y-f(X))^2 = E_X[E_{Y|X}([Y-f(X)]^2|X)]
$$

**우변을 보시면 우선 $X$조건에서 $Y$에 대해 평균화하여 $f(X)$가 fix된 상태에서 원 $Y$값 과의 차이 제곱값을 평균화합니다. 이는 결국 fix되지 않은 $Y$에 대한 함수가 되니까 $X$에 대한 평균을 하면 좌항과 같이 전체 평균이 되는 겁니다. 이를 최소화 하기 위한 $f(X)$값을 찾아야합니다.**

<br>

# 📌 예측 오차 최소화
---

한번 최소화하는 $f(x)$값을 찾아보죠. **예측 오차 식을 풀어써봅시다.**

$$
\begin{align}
E([Y-f(X)]^2|X=x) &= f(X)^2 - 2E(Y|X=x) +E(Y^2|X=x) 
\\ &= (f(X)-E(Y|X=x))^2 +Var(Y|X=x)
\end{align}
$$

가 될텐데요. 결국 해당 식을 가장 최소화 시키는 법은 **`f(X)가 E(Y|X=x)`**과 같아진다면 우변의 첫 항이 없어지므로 최소가 되겠네요. 따라서

$$
f(x) = \arg \min_{f(X)} E_{Y|X}([Y-f(X)]^2|X=x) = E(Y|X=x)
$$

가 됩니다.

이렇기 때문에!! **`맨처음 봣던 E(Y|X=x)가 선형 회귀식의 표현이 되는 겁니다.`**

$$
E[Y|X=x] = \beta_0 + \beta_1x + \epsilon
$$

## 문제

그런데 문제가 있죠. 결국 이 $E[Y\|X=x] = f(X)$를 알려면 **`분포 구조를 알아햐 합니다.`**($(P(x,y))$를 알아야.) 그렇기 때문에 우리가 선형회귀를 사용할 때 **`정규성 가정`**을 하는 것입니다. 분포를 다변량 정규분포라고 가정하면 회귀선이 linear하게 나오기 때문에 선형 회귀라고 하는 겁니다. **문제는 실제로 모든 분포가 정규분포를 따르지는 않을 거에요. 그래서 다양한 모델로 모델링 하는 것입니다.**

<br>

# 절대 오차 loss를 사용한다면
---

loss function으로 square loss가 아닌 **absolute error loss** $L(a,b) = \|a=b\|$를 사용할 때를 한번 봅시다.

위 선형회귀와 똑같이 **$Y, f(X)$**를 대입해준다면

$$
L(X,f(X)) = |Y-f(x)|
$$

이를 풀어 써준다면

$$
E_{Y|X}(|Y-f(X)|X=x) = \int^\mu_{-
\infty}(\mu-y)f_{Y|X=x}(y)dy + \int^\infty_{\mu}(y-\mu)f_{Y|X=x}(y)dy
$$

여기서 $\mu$에 대해 미분해줍시다.

$$
\begin{align}
\dfrac{d}{d\mu} E_{Y|X}(|Y-f(X)|X=x) &= \int^\mu_{-\infty}f_{Y|X=x}(y)dy - \int^\infty_{\mu}f_{Y|X=x}(y)dy 
\\ &= P(Y \le \mu|X=x) - P(Y > \mu|X=x) 
\\ &= 2[P(Y \le \mu|X=x)-\frac12]
\end{align}
$$

해당 식의 우변을 보면, $P(Y \le \mu\|X=x)$는 주어진 $X=x$에서 $Y$가 $\mu$이하일 확률을 나타내고, $P(Y > \mu\|X=x)$는 주어진 $X=x$에서 $Y$가 $\mu$보다 클 확률을 나타냅니다. 그리고 이둘의 차이를 1/2로 뺀 것이 $2[P(Y \le \mu\|X=x)-\frac12]$입니다.

**만약 f(X)가 median(Y\|X=x)**에 가까워 진다면? 즉 **`모델이 Y의 조건부 중앙값을 예측하는 경향이 있다면,`** $P(Y ≤ μ\|X=x), P(Y > μ\|X=x)$ 이둘을 서로 거의 같아 지게 됩니다. 그렇다면 위 식을 0에 가까워 지게 되겠죠. 
그렇기 때문에 **절대 오차 손실 함수를 최소화하는 f(X)가 $median(Y\|X=x)$에 가까워 지게 됩니다.**

$$
f(x) = \arg \min_f(x) E_{Y|X}(|Y- f(X)||X=x) = median(Y|X=x)
$$
