---
excerpt: "편향과 분산은 왜 trade off 관계일까?"

categories :
  - Statistical learning

tags :
  - [통계 학습]

toc: true
toc_sticky: true

date: 2023-09-10
last_modified_at: 2023-09-10

title: "편향과 분산. trade off와 MSE의 관계"
---
<br>
우리는 흔히 편향과 분산이 trade off 관계라고 알고있습니다. 알 사람은 다 알겠지만.. 한번 왜 그런지 확인해보는 시간을 가지려고 합니다
<br>

# 📌 MSE
---

**`우선 MSE에 대해서 봅시다.`**

우리가 MSE에 대해서 이렇게 표현합니다.

$$
MSE = E(y-\hat{f}(\textbf{x}))^2 = Var(\hat{f}(\textbf{x}))+Bias(\hat{f}(\textbf{x}))^2 + Var(\epsilon)
$$

MSE = (실제값과 예측값) 제곱의 기댓값 = 예측값의 분산 + 예측값 편향의 제곱 + 노이즈

이렇게 이루어져 있습니다. 이를 한번 유도해 봅시다.

<br>

## ◾ $Var(\epsilon)$ 유도

여기서 필요한 개념은 $y = f(x) + \epsilon$ 이라는 것 입니다. 
$y$는 어떠한 함수와 우리가 다룰 수 없는 자연적인 nosie로 이루어져 있다는 것입니다.

$$
\begin{align}
E[(y-\hat{f}(\textbf{x}))^2] &= E[(f(\textbf{x})+\epsilon+\hat{f}(\textbf{x}))^2]
\\ &= E[(f(x)-\hat{f}(\textbf{x})^2)] + E[\epsilon^2] + 2*E[(f(x)-\hat{f}(\textbf{x})^2)]*E[\epsilon]
\end{align}
$$

여기서 $\epsilon$의 경우 $\epsilon \sim N(0,\sigma^2)$을 따르므로 기댓값인 $E[\epsilon] = 0$이 됩니다. 따라서

$$
= E[(f(x)-\hat{f}(\textbf{x})^2)] + E[\epsilon^2]
$$

만 남게되고. 이 $E[\epsilon^2]$가 **우리는 다룰수 없는 자연적인 noise $Var(\epsilon)$**이 됩니다.

<br>

## ◾ $Var(\hat{f}(\textbf{x}))+Bias(\hat{f}(\textbf{x}))^2$ 유도


위에서 유도했던

$E[(f(\textbf{x})-\hat{f}(\textbf{x})^2)] + Var(\epsilon)$의 앞 항에 대해 봅시다.

여기서 $\mu$는 $\hat{f}(\textbf{x})$의 기댓값입니다. 작성하기 편하도록 한 것이니 헷갈리지 마세요!

$$
\begin{align}
E[(f(\textbf{x})-\hat{f}(\textbf{x})^2)] &= E[((f(\textbf{x})-\mu + \mu-\hat{f}(\textbf{x}))^2]
\\ &= E[(f(\textbf{x})-\mu)^2 + (\mu -\hat{f}(\textbf{x}))^2 - 2(f(\textbf{x})-\mu)(\mu -\hat{f}(\textbf{x}))]
\\ &= E[(f(\textbf{x})-\mu)^2] + E[(\mu -\hat{f}(\textbf{x}))^2] - 2*E[(f(\textbf{x})-\mu)]*E[(\mu -\hat{f}(\textbf{x}))]
\end{align}
$$

여기서 뒤의 항인 $2\*E[(f(\textbf{x})-\mu)]\*E[(\mu -\hat{f}(\textbf{x}))]$ 만 보자면

$$
\begin{align}
E[(\mu -\hat{f}(\textbf{x}))] &= E[\mu] - E[\hat{f}(\textbf{x})]
\\ &= E[\mu] - \mu
\\ &= \mu - \mu
\\ &= 0
\end{align}
$$

이 됩니다. 앞서 우리는 **$\mu = E[\hat{f}(\textbf{x})]$**라고 했기 때문이죠. 결국 남는 항만 보면

$$
E[(f(\textbf{x})-\mu)^2] + E[(\mu -\hat{f}(\textbf{x}))^2]
\\ = Bias(\hat{f}(\textbf{x}))^2 + Var(\hat{f}(\textbf{x}))
$$

가 남게되어 첫 유도와 두번 째 유도를 합쳐

$$
MSE = Var(\hat{f}(\textbf{x}))+Bias(\hat{f}(\textbf{x}))^2 + Var(\epsilon)
$$

가 나오게 됩니다.

**`여기서 굉장히 중요한 점은`**

- **편향** : $Bias$는 실제 값과 예측값 평균의 차이(예측값이 정답과 얼마나 다른지)
- **분산** : $Var$은 예측값과 예측값 평균의 차이(예측값들끼리 얼마나 흩어져 있는지)
  
라는 것 입니다.

<br>

# 📌 Trade off
---

$$
MSE = E(y-\hat{f}(\textbf{x}))^2 = Var(\hat{f}(\textbf{x}))+Bias(\hat{f}(\textbf{x}))^2 + Var(\epsilon)
$$

해당 식에서 $Var(\epsilon)$의 경우 우리가 어찌 할 수 없는 자연적인 noise이기 때문에 별수 없습니다. 
이를 뺴놓고 구조를 보았을 때 **결국 MSE는 분산(Var)이 커지면 편향이 작아지고(Bias), 반대로 분산(Var)이 작아지면 편향(Bias)는 커지는 Trade off 관계가 있음을 알 수 있습니다.**

<br>

## 쉽게 그래프로 본다면..

![image](https://github.com/novicedata/scrap-comment/assets/88019539/0dbb6cda-b6a1-492a-ba40-4344a6937345){: .align-center}{: width="60%", height="60%"}
*{출처 : http://scott.fortmann-roe.com}*

이처럼 **모델이 단순하다면 분산은 낮고, 편향은 높습니다. 반대로 모델이 복잡하다면 분산은 높고, 편향은 낮아집니다.** 쉽게 아래 그래프로 추가해서 봅시다.

![image](https://github.com/novicedata/scrap-comment/assets/88019539/5f0b2a98-3178-4bdf-a242-5149a3e763d6){: .align-center}
*{출처 : Quora}*

왼쪽 그래프의 경우 간단한 모델입니다. **선이 점들을 많이 지나지 못하죠. 편향이 크고 분산이 낮습니다.** 오른쪽 그래프의 경우 복잡한 모델입니다. **선들이 과적합이 됨을 볼 수 있습니다. 예측 값이 정답과 거의 같을 것입니다. 때문에 편향은 낮고, 분산이 높은 경우 입니다.**

---

> 참조 : [Bias and Variance (편향과 분산) - 한 페이지 머신러닝](https://www.opentutorials.org/module/3653/22071), [[머신러닝] 편향-분산 분해 (Bias-Variance Decomposition)](https://sungkee-book.tistory.com/6), [머신러닝 - 12. 편향(Bias)과 분산(Variance) Trade-off](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-12-%ED%8E%B8%ED%96%A5Bias%EC%99%80-%EB%B6%84%EC%82%B0Variance-Trade-off)
